{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"MovieSummaries/movie.metadata.tsv\", sep = '\\t', \n",
    "                   header = None)\n",
    "print(\"meta.head()\",meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "meta.columns = [\"movie_id\",1,\"movie_name\",3,4,5,6,7,\"genre\"]\n",
    "\n",
    "plots = []\n",
    "\n",
    "with open(\"MovieSummaries/plot_summaries.txt\", 'r') as f:\n",
    "    reader = csv.reader(f, dialect='excel-tab') \n",
    "    for row in tqdm(reader):\n",
    "        plots.append(row)\n",
    "print(\"plots\",plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = []\n",
    "plot = []\n",
    "\n",
    "# extract movie Ids and plot summaries\n",
    "for i in tqdm(plots):\n",
    "    movie_id.append(i[0])\n",
    "    plot.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "movies = pd.DataFrame({'movie_id': movie_id, 'plot': plot})\n",
    "\n",
    "print(\"movies.head()\",movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add the movie names and their genres from the movie metadata file by merging the latter into the former based on the movie_id column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype of 'movie_id'\n",
    "meta['movie_id'] = meta['movie_id'].astype(str)\n",
    "\n",
    "# merge meta with movies\n",
    "movies = pd.merge(movies, meta[['movie_id', 'movie_name', 'genre']], \n",
    "                  on = 'movie_id')\n",
    "\n",
    "print(\"movies.head()\",movies.head())\n",
    "print(\"movies['genre'][0]\",movies['genre'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an empty list\n",
    "genres = [] \n",
    "\n",
    "# extract genres\n",
    "for i in movies['genre']: \n",
    "  genres.append(list(json.loads(i).values())) \n",
    "\n",
    "# add to 'movies' dataframe  \n",
    "movies['genre_new'] = genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the samples might not contain any genre tags. We should remove those samples as they won’t play a part in our model building process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples with 0 genre tags\n",
    "movies_new = movies[~(movies['genre_new'].str.len() == 0)]\n",
    "\n",
    "print(\"movies_new.shape\",movies_new.shape, \"movies.shape\", movies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the genres are now in a list format. Are you curious to find how many movie genres have been covered in this dataset? The below code answers this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all genre tags in a list\n",
    "all_genres = sum(genres,[])\n",
    "\n",
    "print(\"len(set(all_genres)):\", len(set(all_genres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 363 unique genre tags in our dataset. \n",
    "That is quite a big number. I can hardy recall 5-6 genres! \n",
    "Let’s find out what are these tags. We will use FreqDist( ) from the \n",
    "nltk library to create a dictionary of genres and their occurrence count \n",
    "across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = nltk.FreqDist(all_genres) \n",
    "\n",
    "# create dataframe\n",
    "all_genres_df = pd.DataFrame({'Genre': list(all_genres.keys()), \n",
    "                              'Count': list(all_genres.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I personally feel visualizing the data is a much better method than \n",
    "simply putting out numbers. So, let’s plot the distribution of the \n",
    "movie genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = all_genres_df.nlargest(columns=\"Count\", n = 50) \n",
    "plt.figure(figsize=(12,15)) \n",
    "ax = sns.barplot(data=g, x= \"Count\", y = \"Genre\") \n",
    "ax.set(ylabel = 'Count') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will clean our data a bit. I will use some very basic text cleaning steps(as that is not the focus area of this article):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text cleaning \n",
    "def clean_text(text):\n",
    "    # remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    # remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # convert text to lowercase \n",
    "    text = text.lower() \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply the function on the movie plots by using the apply-lambda duo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_new['clean_plot'] = movies_new['plot'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the clean_plot column, all the text is in lowercase and there are also no \n",
    "punctuation marks. Our text cleaning has worked like a charm.\n",
    "\n",
    "The function below will visualize the words and their frequency in a set \n",
    "of documents. Let’s use it to find out the most frequent words in the movie\n",
    "plots column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(x, terms = 30): \n",
    "    all_words = ' '.join([text for text in x]) \n",
    "    all_words = all_words.split() \n",
    "    fdist = nltk.FreqDist(all_words) \n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) \n",
    "  \n",
    "    # selecting top 20 most frequent words \n",
    "    d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "  \n",
    "    # visualize words and frequencies\n",
    "    plt.figure(figsize=(12,15)) \n",
    "    ax = sns.barplot(data=d, x= \"count\", y = \"word\") \n",
    "    ax.set(ylabel = 'Word') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 100 most frequent words \n",
    "freq_words(movies_new['clean_plot'], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the terms in the above plot are stopwords. These stopwords carry far\n",
    "less meaning than other keywords in the text (they just add noise to the data). I’m going to go ahead and remove them from the plots’ text. \n",
    "You can download the list of stopwords from the nltk library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Let’s remove the stopwords:\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "movies_new['clean_plot'] = movies_new['clean_plot'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#Check the most frequent terms sans the stopwords:\n",
    "\n",
    "freq_words(movies_new['clean_plot'], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Text to Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned earlier that we will treat this multi-label classification \n",
    "problem as a Binary Relevance problem. Hence, we will now one hot encode \n",
    "the target variable, i.e., genre_new by using sklearn’s \n",
    "MultiLabelBinarizer( ). Since there are 363 unique genre tags, \n",
    "there are going to be 363 new target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(movies_new['genre_new'])\n",
    "\n",
    "# transform target variable\n",
    "y = multilabel_binarizer.transform(movies_new['genre_new'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it’s time to turn our focus to extracting features from the cleaned \n",
    "version of the movie plots data. For this article, I will be using TF-IDF \n",
    "features. Feel free to use any other feature extraction method you are \n",
    "comfortable with, such as Bag-of-Words, word2vec, GloVe, or ELMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the 10,000 most frequent words in the data as my features. \n",
    "You can try any other number as well for the max_features parameter.\n",
    "\n",
    "Now, before creating TF-IDF features, we will split our data into train and \n",
    "validation sets for training and evaluating our model’s performance. \n",
    "I’m going with a 80-20 split – 80% of the data samples in the train set and \n",
    "the rest in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and validation set\n",
    "xtrain, xval, ytrain, yval = train_test_split(movies_new['clean_plot'], y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create features for the train and the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF features\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\n",
    "xval_tfidf = tfidf_vectorizer.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Movie Genre Prediction Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set for the model building part! This is what we’ve been waiting \n",
    "for.\n",
    "\n",
    "Remember, we will have to build a model for every one-hot encoded target \n",
    "variable. Since we have 363 target variables, we will have to fit \n",
    "363 different models with the same set of predictors (TF-IDF features).\n",
    "\n",
    "As you can imagine, training 363 models can take a considerable amount of \n",
    "time on a modest system. Hence, I will build a Logistic Regression model as \n",
    "it is quick to train on limited computational power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Binary Relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# We will use sk-learn’s OneVsRestClassifier class to solve this problem \n",
    "# as a Binary Relevance or one-vs-all problem:\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf = OneVsRestClassifier(lr)\n",
    "\n",
    "# Finally, fit the model on the train set:\n",
    "# fit model on train data\n",
    "clf.fit(xtrain_tfidf, ytrain)\n",
    "#Predict movie genres on the validation set:\n",
    "# make predictions for validation set\n",
    "y_pred = clf.predict(xval_tfidf)\n",
    "\n",
    "#Let’s check out a sample from these predictions:\n",
    "print(y_pred[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a binary one-dimensional array of length 363. Basically, it is the \n",
    "one-hot encoded form of the unique genre tags. We will have to find a way \n",
    "to convert it into movie genre tags.\n",
    "\n",
    "Luckily, sk-learn comes to our rescue once again. We will use the \n",
    "inverse_transform( ) function along with the MultiLabelBinarizer( ) \n",
    "object to convert the predicted arrays into movie genre tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_binarizer.inverse_transform(y_pred)[3])\n",
    "\n",
    "# evaluate performance\n",
    "f1_score = f1_score(yval, y_pred, average=\"micro\")\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait – we are not done with the problem yet. We also have to take care of \n",
    "the new data or new movie plots that will come in the future, right? Our \n",
    "movie genre prediction system should be able to take a movie plot in raw \n",
    "form as input and generate its genre tag(s).\n",
    "\n",
    "To achieve this, let’s build an inference function. It will take a movie \n",
    "plot text and follow the below steps:\n",
    "\n",
    "* Remove stopwords from the cleaned text\n",
    "* Extract features from the text\n",
    "* Make predictions\n",
    "* Return the predicted movie genre tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tags(q):\n",
    "    q = clean_text(q)\n",
    "    q = remove_stopwords(q)\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    q_pred = clf.predict(q_vec)\n",
    "    return multilabel_binarizer.inverse_transform(q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s test this inference function on a few samples from our validation set:\n",
    "for i in range(5): \n",
    "    k = xval.sample(1).index[0] \n",
    "    print(\"Movie: \", movies_new['movie_name'][k], \n",
    "        \"\\nPredicted genre: \", infer_tags(xval[k]))\n",
    "    print(\"Actual genre: \",movies_new['genre_new'][k], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
